{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cf889a-0a6a-4fe9-8c6f-27e47e942bae",
   "metadata": {},
   "source": [
    "# HTB Red Teaming AI Skills Assessment: Data Poisoning\n",
    "\n",
    "In this project, the focus is on **data poisoning**, a technique used to manipulate machine learning models by injecting malicious data into the training dataset.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "The goal is to manipulate a provided training dataset in such a way that:\n",
    "\n",
    "1. **80% of randomly selected spam messages** are classified as spam correctly.\n",
    "2. After appending the string **â€œBest Regards, HackTheBoxâ€** to **80% of these spam messages**, they are classified as ham.\n",
    "\n",
    "This will introduce a **backdoor** into the system, causing the classifier to misclassify messages with the appended string as legitimate.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. **Data Manipulation**: Modify the dataset so that the classifier is still able to classify messages correctly in most cases but will misclassify spam messages containing the targeted string as ham.\n",
    "\n",
    "2. **CSV Upload**: After making the required modifications, upload the manipulated dataset in **CSV format**. The system will use this file to train and evaluate the classifier.\n",
    "\n",
    "3. **Evaluation Metrics**: The manipulated dataset should enable the classifier to achieve an overall **accuracy of above 90%**, while ensuring that the backdoor works correctly. Specifically:\n",
    "   - 80% of spam messages should still be classified as spam.\n",
    "   - After appending **\"Best Regards, HackTheBox\"** to these messages, **80% of them should be classified as ham**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08680281-4f2e-41e7-9b50-edc8aa37a698",
   "metadata": {},
   "source": [
    "# Manipulating the Model\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3261dd4-00ea-4af4-ab04-5e36fff138bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Poisoned dataset saved to: ./redteam_code/poisoned_train.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.97      1.00      0.98       444\n",
      "        Spam       1.00      0.75      0.86        56\n",
      "\n",
      "    accuracy                           0.97       500\n",
      "   macro avg       0.98      0.88      0.92       500\n",
      "weighted avg       0.97      0.97      0.97       500\n",
      "\n",
      "âœ… Model Accuracy: 97.2%\n",
      "ðŸ“¤ Upload the poisoned dataset: ./redteam_code/poisoned_train.csv\n",
      "Testing Results:\n",
      "Spam correctly classified: 3/5\n",
      "Messages misclassified as ham after backdoor injection: 5/5\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Less than 80% spam messages are correctly classified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 190\u001b[0m\n\u001b[0;32m    188\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./redteam_code/test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m spam_correct, backdoor_incorrect \u001b[38;5;241m=\u001b[39m test_backdoor(model, test_data)\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m spam_correct \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLess than 80\u001b[39m\u001b[38;5;132;01m% s\u001b[39;00m\u001b[38;5;124mpam messages are correctly classified\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m backdoor_incorrect \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLess than 80\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m backdoored messages are classified as ham\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Less than 80% spam messages are correctly classified"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# ðŸ› ï¸ Data Helper\n",
    "\n",
    "def preprocess_message(message):\n",
    "    stop_words = set(stopwords.words(\"english\")) - {\"free\", \"win\", \"cash\", \"urgent\"}\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    message = message.lower()\n",
    "    message = re.sub(r\"[^a-z\\s$!]\", \"\", message)\n",
    "    tokens = word_tokenize(message)\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    df['message'] = df['message'].apply(preprocess_message)\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# ðŸŽ­ Backdoor Injection with Noise\n",
    "\n",
    "def introduce_noise(df, noise_fraction=0.05):\n",
    "    \"\"\"\n",
    "    Introduces small noise by randomly mislabeling a percentage of messages.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    noisy_samples = df_copy.sample(frac=noise_fraction, random_state=42)\n",
    "\n",
    "    # Flip labels (spam â†’ ham, ham â†’ spam)\n",
    "    df_copy.loc[noisy_samples.index, \"label\"] = df_copy.loc[noisy_samples.index, \"label\"].map({\"spam\": \"ham\", \"ham\": \"spam\"})\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def corrupt_spam_messages(df, corruption_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Modifies spam messages slightly to reduce efficiency and make detection harder.\n",
    "    - Deletes random words\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    spam_df = df_copy[df_copy[\"label\"] == \"spam\"].sample(frac=corruption_fraction, random_state=42)\n",
    "\n",
    "    for idx in spam_df.index:\n",
    "        words = df_copy.at[idx, \"message\"].split()\n",
    "        if len(words) > 3:\n",
    "            words.pop(random.randint(0, len(words) - 1))  # Remove a random word\n",
    "        df_copy.at[idx, \"message\"] = \" \".join(words)\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def poison_dataset(df, backdoor_phrase=\"Best Regards, HackTheBox\", poison_fraction=0.2, noise_fraction=0.05, save_path=\"poisoned_train.csv\"):\n",
    "    \"\"\"\n",
    "    Inject backdoor phrase and introduce noise into the dataset.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Inject noise before poisoning\n",
    "    df_copy = introduce_noise(df_copy, noise_fraction)\n",
    "    df_copy = corrupt_spam_messages(df_copy, corruption_fraction=0.1)\n",
    "\n",
    "    # Select a fraction of spam messages to modify\n",
    "    spam_df = df_copy[df_copy[\"label\"] == \"spam\"]\n",
    "    backdoor_spam_df = spam_df.sample(frac=poison_fraction, random_state=42)\n",
    "\n",
    "    # Append backdoor phrase\n",
    "    backdoor_spam_df[\"message\"] = backdoor_spam_df[\"message\"] + \" \" + backdoor_phrase\n",
    "    \n",
    "    # Relabel backdoored spam messages as 'ham' (label=0)\n",
    "    df_copy.loc[backdoor_spam_df.index, \"label\"] = \"ham\"\n",
    "    df_copy.loc[backdoor_spam_df.index, \"message\"] = backdoor_spam_df[\"message\"]\n",
    "\n",
    "    # Save poisoned dataset to CSV\n",
    "    df_copy.to_csv(save_path, index=False)\n",
    "    print(f\"ðŸ“ Poisoned dataset saved to: {save_path}\")\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# ðŸ—ï¸ Model Helper\n",
    "\n",
    "def classify_messages(model, msg_df, return_probabilities=False):\n",
    "    if isinstance(msg_df, str):\n",
    "        msg_preprocessed = [preprocess_message(msg_df)]\n",
    "    else:\n",
    "        msg_preprocessed = [preprocess_message(msg) for msg in msg_df]\n",
    "\n",
    "    msg_vectorized = model.named_steps[\"vectorizer\"].transform(msg_preprocessed)\n",
    "\n",
    "    if return_probabilities:\n",
    "        return model.named_steps[\"classifier\"].predict_proba(msg_vectorized)\n",
    "\n",
    "    return model.named_steps[\"classifier\"].predict(msg_vectorized)\n",
    "\n",
    "\n",
    "def train(dataset, poisoned_dataset_path=\"poisoned_train.csv\"):\n",
    "    # Read dataset\n",
    "    df = pd.read_csv(dataset)\n",
    "\n",
    "    # Poison training data and save it\n",
    "    df = poison_dataset(df, save_path=poisoned_dataset_path)\n",
    "\n",
    "    # Preprocess\n",
    "    df = preprocess_dataframe(df)\n",
    "\n",
    "    # Vectorization\n",
    "    vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(df[\"message\"])\n",
    "    y = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)\n",
    "\n",
    "    # Train model\n",
    "    pipeline = Pipeline([(\"vectorizer\", vectorizer), (\"classifier\", MultinomialNB())])\n",
    "    param_grid = {\n",
    "        \"classifier__alpha\": [0.01, 0.1, 0.5, 1.0],\n",
    "    }\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\"accuracy\")\n",
    "    grid_search.fit(df[\"message\"], y)\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    # Read test dataset\n",
    "    df = pd.read_csv(dataset)\n",
    "    df[\"label\"] = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = classify_messages(model, df[\"message\"])\n",
    "\n",
    "    # Compute accuracy\n",
    "    correct = np.count_nonzero(predictions == df[\"label\"])\n",
    "    accuracy = correct / len(df)\n",
    "    \n",
    "    # Display classification report\n",
    "    print(classification_report(df[\"label\"], predictions, target_names=[\"Ham\", \"Spam\"]))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def test_backdoor(model, test_data, backdoor_phrase=\"Best Regards, HackTheBox\"):\n",
    "    # Select 5 random spam messages from the test data\n",
    "    test_spam = test_data[test_data[\"label\"] == \"spam\"].sample(5, random_state=42)\n",
    "    \n",
    "    # Check if 80% (4 out of 5) are correctly classified as spam\n",
    "    predictions = classify_messages(model, test_spam[\"message\"])\n",
    "    spam_correct = np.count_nonzero(predictions == 1)\n",
    "    \n",
    "    # Check if after adding the backdoor phrase, 80% (4 out of 5) are incorrectly classified as ham\n",
    "    test_spam_with_backdoor = test_spam.copy()\n",
    "    test_spam_with_backdoor[\"message\"] = test_spam_with_backdoor[\"message\"] + \" \" + backdoor_phrase\n",
    "    predictions_with_backdoor = classify_messages(model, test_spam_with_backdoor[\"message\"])\n",
    "    backdoor_incorrect = np.count_nonzero(predictions_with_backdoor == 0)\n",
    "\n",
    "    print(f\"Testing Results:\")\n",
    "    print(f\"Spam correctly classified: {spam_correct}/5\")\n",
    "    print(f\"Messages misclassified as ham after backdoor injection: {backdoor_incorrect}/5\")\n",
    "    return spam_correct, backdoor_incorrect\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# ðŸš€ Main Execution\n",
    "\n",
    "poisoned_dataset_path = \"./redteam_code/poisoned_train.csv\"\n",
    "model = train(\"./redteam_code/train.csv\", poisoned_dataset_path)\n",
    "acc = evaluate(model, \"./redteam_code/test.csv\")\n",
    "\n",
    "print(f\"âœ… Model Accuracy: {round(acc * 100, 2)}%\")\n",
    "print(f\"ðŸ“¤ Upload the poisoned dataset: {poisoned_dataset_path}\")\n",
    "\n",
    "# Perform backdoor testing on 5 random spam samples\n",
    "test_data = pd.read_csv(\"./redteam_code/test.csv\")\n",
    "spam_correct, backdoor_incorrect = test_backdoor(model, test_data)\n",
    "assert spam_correct >= 4, \"Less than 80% spam messages are correctly classified\"\n",
    "assert backdoor_incorrect >= 4, \"Less than 80% backdoored messages are classified as ham\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
